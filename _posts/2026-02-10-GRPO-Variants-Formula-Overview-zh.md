---
layout: post
title: 一图串讲GRPO十几种主流变体算法
date: 2026-02-10 21:01:00
description: 一图串讲GRPO十几种主流变体算法
categories: reinforcement-learning
---


<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/GRPO-Variants.JPG" class="img-fluid rounded z-depth-1" %}
    </div>
</div>


**事先声明**：本文主要从目标公式的角度，尝试归纳并梳理GRPO算法的一些主流变体。为突出重点，部分背景知识已略去，同时涵盖的内容也难免有疏漏之处，还望读者理解。

---

**图片说明**：

1.  **黑色公式**：GRPO 的完整目标函数，尽量沿用原论文符号。
2.  **蓝色与橙色**：分别表示改进思路与对应的算法名称。
3.  **采样细节**：公式中期望 $\mathbb{E}$ 内的采样来自行为策略 $\mu_{\text{old}}$ ，而非参考策略 $\pi_{\text{old}}$，这是为了强调训练与推理时的不一致性。后续不少算法都针对此问题进行了改进。
4.  **DAPO的采样改进**：DAPO 在采样阶段过滤了全 0、全 1 的样本，因为这些样本对梯度更新没有贡献。它也排除了因 `max_len` 截断的样本，认为这些样本语义不完整，用于更新只会引入噪声。
5.  **DAPO的求和改进**：原 GRPO 公式先对序列内的 token 权重取平均，这在一定程度上降低了长序列样本中每个 token 的总体重要性。DAPO 对此进行了改进，对不同长度序列中的 token 权重一视同仁。
6.  **求和与连乘**：仔细观察 token 级别的求和符号 $\sum$，其实它是有偏差的。熟悉传统序列模型（如 CRF、HMM）的读者知道，它们对整体序列概率的建模一般采用连乘 $\prod$ 形式，而这里的连加 $\sum$ 显得有些“不自然”。可以证明，连加形式仅是连乘形式的一阶泰勒近似。因此，**GSPO** 与 **GMPO** 都选择了连乘形式建模，从而降低了理论偏差。两者的细微区别在于：GMPO 先裁剪后连乘，而 GSPO 先连乘后裁剪。
7.  **训练-推理不一致的改进**：公式中的 $\frac{\pi_{\text{old}}}{\mu_{\text{old}}}$ 是训练与推理不一致的关键环节，模型训练崩溃常与此有关。该比值越偏离 1，训练越不稳定。各改进策略如下：
    *   **IcePOP**：类似 Clip 操作，直接舍弃显著偏离的 token。
    *   **TIS**：对该比值进行平滑。
    *   **MIS/WTRS**：对整个序列的训推不一致指标进行评估，并舍弃严重不一致的整条序列。
8.  **Min-Clip 操作**：“min-clip” 是作者对 min 与 clip 操作的简写。进一步分析，对 $\frac{\pi_\theta}{\pi_{\text{old}}}$ 直接进行硬裁剪会浪费本就不多的有效样本，不利于探索。因此，**CISPO** 和 **SAPO** 都致力于对 clip 操作进行平滑化改进。这种改进思路类似于 Hinge Loss 与 Log Loss 的区别。
9.  **CISPO 思路**：保留梯度，引入停止梯度（stop-gradient, sg）操作。为保证在非裁剪区域的梯度一致，增加了 $\log(\pi_\theta)$ 项进行配平。
10. **SAPO 思路**：直接使用 Log Loss 中的 Sigmoid 函数进行平滑，其中 $\tau$ 为温度系数。为确保 $\frac{\pi_\theta}{\pi_{\text{old}}}$ 在 1 附近的梯度不变，引入了 $\frac{4}{\tau}$ 进行配平。
11. **小结**：
    *   $\frac{\pi_\theta}{\pi_{\text{old}}}$ 关注的是 **策略新旧不一致**。裁剪太少会使代理目标偏离真实目标，导致训练崩溃；裁剪太多则会抑制探索，容易过拟合。
    *   $\frac{\pi_{\text{old}}}{\mu_{\text{old}}}$ 关注的是 **训练与推理不一致**。裁剪太少易导致训练崩溃，裁剪太多也可能不利于探索。
    *   两者作用不同，可以解耦处理。现有改进策略多在 **裁剪方式** 上做文章，例如：硬裁剪 vs 软裁剪、Token 级裁剪 vs 序列级裁剪、对称裁剪 vs 非对称裁剪。例如，DAPO 建议直接提高裁剪上限，就是一种典型的非对称裁剪。
12. **补充说明**：CISPO 和 SAPO 都是对 min-clip 中的 clip 部分进行改进，而 **舍弃了 min 操作**。这并非不合理，但会导致它们的函数图像在部分区域与 min-clip 的图像趋势不一致，分析时请注意。
13. **针对 MOE 模型的改进**：对于混合专家模型，上述两种不一致性会被路由机制显著放大。注意到 $\frac{\pi_\theta}{\mu_{\text{old}}}=\frac{\pi_\theta}{\pi_{\text{old}}} \times \frac{\pi_{\text{old}}}{\mu_{\text{old}}}$。由此产生两种思路：
    *   **R2**：将 $\pi_\theta$ 中的专家固定为 $\pi_{\text{old}}$ 中的专家，以降低新旧不一致。
    *   **R3**：将 $\pi_\theta$ 中的专家约束为可学习的，但使其趋向于 $\mu_{\text{old}}$ 中的专家，以同时降低两种不一致性。
    *   需要注意的是，这些路由回放策略也降低了模型的容量，可能损害其拟合性能。
14. **奖励分数估计**：原始 GRPO 公式中的归一化分母（标准差 std）可证明是有偏的，因此 **Dr.GRPO** 选择直接去掉它。
15. **奖励塑形**：针对单个样本的奖励设计也有不少工作，例如 **DAPO** 专门为过长的序列样本设计了一个软性惩罚，实验证明有一定效果。
16. **关于 KL 散度正则项**：本文分析未包含 KL 散度正则项，主要将其视为与原始目标函数解耦的一个独立策略进行处理。事实上，部分论文会直接放弃此项。由于 KL 散度有多种估计和应用方式，笔者将另文介绍。
17. 高清原图可到github上代码仓库下载。
