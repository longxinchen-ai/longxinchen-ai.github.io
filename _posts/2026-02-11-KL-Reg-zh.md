---
layout: post
title: 一张表串讲LLM-RL中KL散度正则的正确与错误用法
date: 2026-02-11 23:01:00
description: 一张表串讲LLM-RL中KL散度正则的正确与错误用法
categories: reinforcement-learning
---


<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/KL-Reg.PNG" class="img-fluid rounded z-depth-1" %}
    </div>
</div>


**声明**：为突出重点，部分背景知识已略去，同时涵盖的内容也难免有疏漏之处，还望读者理解。

---

## 背景说明

1.  KL散度正则化涉及多种决策项，包括：使用正向KL散度还是逆向KL散度（2种）、将其置于损失函数还是奖励函数中（2种）、采用同策略还是异策略（2种）、选择k1/k2/k3中的哪种形式（3种）、以及分析目标和梯度各自的偏差与方差（2×2种）。简单计算有 $2×2×2×3×2×2=96$ 种组合。当然，实际分析无需覆盖全部。例如，本文聚焦于当前主流的逆向KL散度，情况数便减少一半。若偏差分析已证实存在偏差，方差分析也可省略。

2.  无论作何选择，**最关键的是保证实际训练中KL散度梯度的偏差和方差足够小。而对KL散度求梯度，是整个分析中最需留意的关键点。**

4.  从数学上讲，KL散度本质是一个期望，而期望的本质是积分。对KL散度求梯度，就是对积分求梯度，这需要用到莱布尼茨积分求导法则。具体来说，已知随机变量 $f(x; \theta)$ 在分布 $q(x; \theta)$ 下的期望 $\mathbb{E}_{x \sim q(x; \theta)}[f(x; \theta)]$，对其求参数 $\theta$ 的梯度，需**分别对 $f$ 和 $q$ 求梯度**，结果分别称为“路径梯度”和“得分函数”。公式表达为：
    $$
    \nabla_{\theta} \mathbb{E}_{x \sim q(x; \theta)}[f(x; \theta)] = \mathbb{E}_{x \sim q(x; \theta)} \left[ \nabla_{\theta} f(x; \theta) + f(x; \theta) \, \nabla_{\theta} \log q(x; \theta) \right]
    $$

5.  对于逆向KL散度 $\mathbb{E}_q [k_1]$，套用上述梯度公式，其路径梯度项为零，仅剩得分函数项 $\mathbb{E}_q [s k_1]$。这两项即表格中第0行列出的真实（True）KL散度目标项（第二列）与梯度项（第四列）。

6.  在代码实现中，对期望（无论损失还是梯度）的计算均采用蒙特卡洛估计。期望符号 $\mathbb{E}$ 右下角的分布 $q(x)$ 仅用于采样，并未在计算图中显式出现。这对损失值计算影响不大，但求梯度时则不同：反向传播不会对采样分布 $q(x)$ 求梯度，因而会遗漏重要的得分函数项。因此，核心问题在于，**确保代码实现与理论分析两种方式求得的梯度尽量相等（无偏）**，并在此基础上降低方差。

7.  为阐明此问题，表格分为4列：
    *   **第一列**：代码实现意义上的目标表达式。此列中所有期望 $\mathbb{E}$ 右下角的采样分布**不会**被求梯度，仅正右方的随机变量部分参与梯度计算。
    *   **第三列**：对第一列表达式直接**求梯度**（在代码意义下）得到的结果。
    *   **第二、四列**：为便于理论分析，分别对第一、三列中的表达式**求期望**（此时采样分布生效），以进一步化简。若它们的期望与真实的KL散度目标及梯度的期望一致，则是无偏的。
    *   **表头蓝色箭头**说明了列间关系（求梯度或求期望）。
    *   简言之，**第一列求梯度时须忽略 $\mathbb{E}$ 下的分布 $q(x)$，而第二、四列求期望时则须让 $q(x)$ 生效。**

8.  本文工作建立在诸多前人研究基础上，相关文献列于文末，大部分公式推导均可从中找到。

## KL散度作为损失（朴素同策略实现）

1.  在同策略下，采样分布即为KL散度分子部分的分布 $q$。为强调其在代码反向传播中**不被**求梯度，**只对 $k_*$ 求梯度**，表格第一列前三行用 $$\mathbb{E}_{sg(q)}[·] $$ 表示。
2.  第一行对k1推导，对 $$\mathbb{E}_{sg(q)}[k_1]$$ 求梯度，得到第三列的 $$\mathbb{E}_q[s]$$。第二列是第一列的数学期望，结果为 $\mathbb{E}_q [k_1]$，与真实KL散度目标项一致，故目标**无偏**。第四列是第三列的数学期望，结果为 $0$（这是一个重要性质）。公式推导需将期望还原为积分形式，此处仅分析结果：$0$ 与真实梯度项 $\mathbb{E}_q [s k_1]$ 不一致，故梯度**有偏**。
3.  第二行推导类似。因 $k_2 = \frac{1}{2} k_1^2$，其第三列梯度结果恰好是 $\mathbb{E}_{sg(q)}[s k_1]$，其期望（第四列）为 $\mathbb{E}_q [s k_1]$，与真实梯度项一致，故梯度**无偏**。
4.  第三行推导前面类似，都是**只对 $k_*$ 求梯度**，但需注意其第四列梯度结果是正向KL散度的梯度，与真实（逆向KL）梯度项不一致，故**有偏**。
5.  **小结**：$k_1$/$k_3$ 作为目标时无偏，但其梯度有偏；$k_2$ 则相反，目标有偏（但偏差低），梯度无偏。

## KL散度作为损失（统一同/异策略实现）

1.  在异策略下，采样分布 $\mu$ 不必与目标分布 $q$ 相同，只需在随机变量部分乘上重要性权重 $\frac{q}{\mu}$ 即可。$\mu$ 不被求梯度，而 $q$ 可以。因此可以**同时对 $q$ 与 $k_*$ 求梯度**，可同时得到路径梯度和得分函数，分析更为顺利。
2.  若令 $\mu = sg(q)$，则转换为一种可对 $q$ 求梯度的特殊同策略形式。因此，此形式可统一分析同策略与异策略。
3.  第四行推导：对第一列k1的表达式求梯度得到第三列（**同时对 $q$ 与 $k_*$ 求梯度**），再求期望得到第四列（利用 $\mathbb{E}_q [s] = 0$ 的性质），结果无偏。
4.  第五行推导类似，发现梯度求期望后无法化简，有偏。
5.  第六行推导类似，梯度求期望后无偏。
6.  **小结**：$k_1$/$k_3$ 无论作为目标还是求梯度，都**无偏**；$k_2$ 无论作为目标还是求梯度，都**有偏**。

## KL散度作为奖励惩罚（统一同/异策略实现）

1.  奖励在目标函数中同样需乘以重要性权重 $\frac{q}{\mu}$。关键区别在于，奖励项本身不作为函数被求梯度，因此作**为奖励惩罚的KL散度整体不被求梯度**。故表格第一列后三行用 $\mathbb{E}_{\mu} \left[ \frac{q}{\mu} sg(\cdot) \right]$ 表示，仅重要性权重中的分子 $q$ 能被求梯度。
2.  若令 $\mu = sg(q)$，则转换为可对 $q$ 求梯度的同策略形式，同时保持了理论分析的统一性。
3.  第七行推导：对第一列k1的表达式求梯度得到第三列（**只对 $q$ 求梯度**），再求期望得到第四列，结果无偏。
4.  第八第九行推导类似，均只对 $q$ 求梯度，$k_*$ 的形式不变，得到梯度是有偏的。
5.  **小结**：$k_1$ 作为目标与求梯度均**无偏**；$k_2$/$k_3$ 求梯度均**有偏**。

## 对目标统一进行分析

1.  $k_1$, $k_2$, $k_3$ 无论是作为损失还是奖励，无论同策略或异策略，其**目标值**的行为是一致的。
2.  **偏差角度**：$k_1$/$k_3$ 无偏，$k_2$ 低偏差。
3.  **方差角度**：$k_1$ 高方差，$k_2$/$k_3$ 低方差。因为 $k_1$ 在0附近是一阶近似，而 $k_2$ 和 $k_3$ 是二阶近似。

## 对梯度统一进行分析

1.  以下三种情况梯度行为一致，均**无偏**且方差相同，可作为推荐策略：
    *   $k_1$ 作为奖励时
    *   $k_2$ 作为损失时（朴素同策略实现）
    *   $k_3$ 作为损失时（统一同/异策略实现）
2.  $k_1$ 作为损失时（统一同/异策略实现），其梯度虽与上述情况一致（无偏），但**方差更高**（多出一项零偏差非零方差的项）。
3.  除以上列出的情况外，其他使用方式在梯度上均存在偏差，属于**错误方法**。

## 参考文献
1.  Xihuai Wang, Shao Zhang. "Understanding KL Divergence Estimators in RL: From Value Approximation to Gradient Estimation". <https://xihuai18.github.io/reinforcement-learning/2025/12/01/kl-estimators-en.html>

2. Dibya Ghosh. "KL Divergence for Machine Learning". <https://dibyaghosh.com/blog/probability/kldivergence>

3. John Schulman. "Approximating KL Divergence". <https://joschu.net/blog/kl-approx.html>

4. Kezhao Liu, Jason Klein Liu, Mingtao Chen, Yiming Liu. "Rethinking KL Regularization in RLHF: From Value Estimation to Gradient Optimization". <https://arxiv.org/abs/2510.01555>

5. Yifan Zhang, Yiping Ji, Gavin Brown, et al. "On the Design of KL-Regularized Policy Gradient Algorithms for LLM Reasoning". <https://arxiv.org/abs/2505.17508>

6. Vedant Shah, Johan Obando-Ceron, Vineet Jain, Brian Bartoldson, Bhavya Kailkhura, Sarthak Mittal, Glen Berseth, Pablo Samuel Castro. "A Comedy of Estimators: On KL Regularization in RL Training of LLMs". <https://arxiv.org/abs/2512.21852>

