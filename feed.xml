<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://longxinchen-ai.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://longxinchen-ai.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-02-11T06:49:03+00:00</updated><id>https://longxinchen-ai.github.io/feed.xml</id><title type="html">Longxinchen’s Page</title><subtitle></subtitle><entry><title type="html">一图串讲GRPO十几种主流变体算法</title><link href="https://longxinchen-ai.github.io/blog/2026/GRPO-Variants-Formula-Overview-zh/" rel="alternate" type="text/html" title="一图串讲GRPO十几种主流变体算法"/><published>2026-02-10T21:01:00+00:00</published><updated>2026-02-10T21:01:00+00:00</updated><id>https://longxinchen-ai.github.io/blog/2026/GRPO-Variants-Formula-Overview-zh</id><content type="html" xml:base="https://longxinchen-ai.github.io/blog/2026/GRPO-Variants-Formula-Overview-zh/"><![CDATA[<div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/GRPO-Variants.JPG" sizes="95vw"/> <img src="/assets/img/GRPO-Variants.JPG" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong>事先声明</strong>：本文主要从目标公式的角度，尝试归纳并梳理GRPO算法的一些主流变体。为突出重点，部分背景知识已略去，同时涵盖的内容也难免有疏漏之处，还望读者理解。</p> <hr/> <p><strong>图片说明</strong>：</p> <ol> <li><strong>黑色公式</strong>：GRPO 的完整目标函数，尽量沿用原论文符号。</li> <li><strong>蓝色与橙色</strong>：分别表示改进思路与对应的算法名称。</li> <li><strong>采样细节</strong>：公式中期望 $\mathbb{E}$ 内的采样来自行为策略 $\mu_{\text{old}}$ ，而非参考策略 $\pi_{\text{old}}$，这是为了强调训练与推理时的不一致性。后续不少算法都针对此问题进行了改进。</li> <li><strong>DAPO的采样改进</strong>：DAPO 在采样阶段过滤了全 0、全 1 的样本，因为这些样本对梯度更新没有贡献。它也排除了因 <code class="language-plaintext highlighter-rouge">max_len</code> 截断的样本，认为这些样本语义不完整，用于更新只会引入噪声。</li> <li><strong>DAPO的求和改进</strong>：原 GRPO 公式先对序列内的 token 权重取平均，这在一定程度上降低了长序列样本中每个 token 的总体重要性。DAPO 对此进行了改进，对不同长度序列中的 token 权重一视同仁。</li> <li><strong>求和与连乘</strong>：仔细观察 token 级别的求和符号 $\sum$，其实它是有偏差的。熟悉传统序列模型（如 CRF、HMM）的读者知道，它们对整体序列概率的建模一般采用连乘 $\prod$ 形式，而这里的连加 $\sum$ 显得有些“不自然”。可以证明，连加形式仅是连乘形式的一阶泰勒近似。因此，<strong>GSPO</strong> 与 <strong>GMPO</strong> 都选择了连乘形式建模，从而降低了理论偏差。两者的细微区别在于：GMPO 先裁剪后连乘，而 GSPO 先连乘后裁剪。</li> <li><strong>训练-推理不一致的改进</strong>：公式中的 $\frac{\pi_{\text{old}}}{\mu_{\text{old}}}$ 是训练与推理不一致的关键环节，模型训练崩溃常与此有关。该比值越偏离 1，训练越不稳定。各改进策略如下： <ul> <li><strong>IcePOP</strong>：类似 Clip 操作，直接舍弃显著偏离的 token。</li> <li><strong>TIS</strong>：对该比值进行平滑。</li> <li><strong>MIS/WTRS</strong>：对整个序列的训推不一致指标进行评估，并舍弃严重不一致的整条序列。</li> </ul> </li> <li><strong>Min-Clip 操作</strong>：“min-clip” 是作者对 min 与 clip 操作的简写。进一步分析，对 $\frac{\pi_\theta}{\pi_{\text{old}}}$ 直接进行硬裁剪会浪费本就不多的有效样本，不利于探索。因此，<strong>CISPO</strong> 和 <strong>SAPO</strong> 都致力于对 clip 操作进行平滑化改进。这种改进思路类似于 Hinge Loss 与 Log Loss 的区别。</li> <li><strong>CISPO 思路</strong>：保留梯度，引入停止梯度（stop-gradient, sg）操作。为保证在非裁剪区域的梯度一致，增加了 $\log(\pi_\theta)$ 项进行配平。</li> <li><strong>SAPO 思路</strong>：直接使用 Log Loss 中的 Sigmoid 函数进行平滑，其中 $\tau$ 为温度系数。为确保 $\frac{\pi_\theta}{\pi_{\text{old}}}$ 在 1 附近的梯度不变，引入了 $\frac{4}{\tau}$ 进行配平。</li> <li><strong>小结</strong>： <ul> <li>$\frac{\pi_\theta}{\pi_{\text{old}}}$ 关注的是 <strong>策略新旧不一致</strong>。裁剪太少会使代理目标偏离真实目标，导致训练崩溃；裁剪太多则会抑制探索，容易过拟合。</li> <li>$\frac{\pi_{\text{old}}}{\mu_{\text{old}}}$ 关注的是 <strong>训练与推理不一致</strong>。裁剪太少易导致训练崩溃，裁剪太多也可能不利于探索。</li> <li>两者作用不同，可以解耦处理。现有改进策略多在 <strong>裁剪方式</strong> 上做文章，例如：硬裁剪 vs 软裁剪、Token 级裁剪 vs 序列级裁剪、对称裁剪 vs 非对称裁剪。例如，DAPO 建议直接提高裁剪上限，就是一种典型的非对称裁剪。</li> </ul> </li> <li><strong>补充说明</strong>：CISPO 和 SAPO 都是对 min-clip 中的 clip 部分进行改进，而 <strong>舍弃了 min 操作</strong>。这并非不合理，但会导致它们的函数图像在部分区域与 min-clip 的图像趋势不一致，分析时请注意。</li> <li><strong>针对 MOE 模型的改进</strong>：对于混合专家模型，上述两种不一致性会被路由机制显著放大。注意到 $\frac{\pi_\theta}{\mu_{\text{old}}}=\frac{\pi_\theta}{\pi_{\text{old}}} \times \frac{\pi_{\text{old}}}{\mu_{\text{old}}}$。由此产生两种思路： <ul> <li><strong>R2</strong>：将 $\pi_\theta$ 中的专家固定为 $\pi_{\text{old}}$ 中的专家，以降低新旧不一致。</li> <li><strong>R3</strong>：将 $\pi_\theta$ 中的专家约束为可学习的，但使其趋向于 $\mu_{\text{old}}$ 中的专家，以同时降低两种不一致性。</li> </ul> </li> <li><strong>奖励分数估计</strong>：原始 GRPO 公式中的归一化分母（标准差 std）可证明是有偏的，因此 <strong>Dr.GRPO</strong> 选择直接去掉它。</li> <li><strong>奖励塑形</strong>：针对单个样本的奖励设计也有不少工作，例如 <strong>DAPO</strong> 专门为过长的序列样本设计了一个软性惩罚，实验证明有一定效果。</li> <li><strong>关于 KL 散度正则项</strong>：本文分析未包含 KL 散度正则项，主要将其视为与原始目标函数解耦的一个独立策略进行处理。事实上，部分论文会直接放弃此项。由于 KL 散度有多种估计和应用方式，笔者将另文介绍。</li> <li>高清原图可到github上代码仓库下载。</li> </ol>]]></content><author><name></name></author><category term="reinforcement-learning"/><summary type="html"><![CDATA[一图串讲GRPO十几种主流变体算法]]></summary></entry></feed>