<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://longxinchen-ai.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://longxinchen-ai.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-02-12T06:38:32+00:00</updated><id>https://longxinchen-ai.github.io/feed.xml</id><title type="html">Longxinchen’s Page</title><subtitle></subtitle><entry><title type="html">一张表串讲LLM-RL中KL散度正则的正确与错误用法</title><link href="https://longxinchen-ai.github.io/blog/2026/KL-Reg-zh/" rel="alternate" type="text/html" title="一张表串讲LLM-RL中KL散度正则的正确与错误用法"/><published>2026-02-11T23:01:00+00:00</published><updated>2026-02-11T23:01:00+00:00</updated><id>https://longxinchen-ai.github.io/blog/2026/KL-Reg-zh</id><content type="html" xml:base="https://longxinchen-ai.github.io/blog/2026/KL-Reg-zh/"><![CDATA[<div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/KL-Reg.PNG" sizes="95vw"/> <img src="/assets/img/KL-Reg.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong>声明</strong>：为突出重点，部分背景知识已略去，同时涵盖的内容也难免有疏漏之处，还望读者理解。</p> <hr/> <h2 id="背景说明">背景说明</h2> <ol> <li> <p>KL散度正则化涉及多种决策项，包括：使用正向KL散度还是逆向KL散度（2种）、将其置于损失函数还是奖励函数中（2种）、采用同策略还是异策略（2种）、选择k1/k2/k3中的哪种形式（3种）、以及分析目标和梯度各自的偏差与方差（2×2种）。简单计算有 $2×2×2×3×2×2=96$ 种组合。当然，实际分析无需覆盖全部。例如，本文聚焦于当前主流的逆向KL散度，情况数便减少一半。若偏差分析已证实存在偏差，方差分析也可省略。</p> </li> <li> <p>无论作何选择，<strong>最关键的是保证实际训练中KL散度梯度的偏差和方差足够小。而对KL散度求梯度，是整个分析中最需留意的关键点。</strong></p> </li> <li> <p>从数学上讲，KL散度本质是一个期望，而期望的本质是积分。对KL散度求梯度，就是对积分求梯度，这需要用到莱布尼茨积分求导法则。具体来说，已知随机变量 $f(x; \theta)$ 在分布 $q(x; \theta)$ 下的期望 $\mathbb{E}_{x \sim q(x; \theta)}[f(x; \theta)]$，对其求参数 $\theta$ 的梯度，需<strong>分别对 $f$ 和 $q$ 求梯度</strong>，结果分别称为“路径梯度”和“得分函数”。公式表达为： \(\nabla_{\theta} \mathbb{E}_{x \sim q(x; \theta)}[f(x; \theta)] = \mathbb{E}_{x \sim q(x; \theta)} \left[ \nabla_{\theta} f(x; \theta) + f(x; \theta) \, \nabla_{\theta} \log q(x; \theta) \right]\)</p> </li> <li> <p>对于逆向KL散度 $\mathbb{E}_q [k_1]$，套用上述梯度公式，其路径梯度项为零，仅剩得分函数项 $\mathbb{E}_q [s k_1]$。这两项即表格中第0行列出的真实（True）KL散度目标项（第二列）与梯度项（第四列）。</p> </li> <li> <p>在代码实现中，对期望（无论损失还是梯度）的计算均采用蒙特卡洛估计。期望符号 $\mathbb{E}$ 右下角的分布 $q(x)$ 仅用于采样，并未在计算图中显式出现。这对损失值计算影响不大，但求梯度时则不同：反向传播不会对采样分布 $q(x)$ 求梯度，因而会遗漏重要的得分函数项。因此，核心问题在于，<strong>确保代码实现与理论分析两种方式求得的梯度尽量相等（无偏）</strong>，并在此基础上降低方差。</p> </li> <li>为阐明此问题，表格分为4列： <ul> <li><strong>第一列</strong>：代码实现意义上的目标表达式。此列中所有期望 $\mathbb{E}$ 右下角的采样分布<strong>不会</strong>被求梯度，仅正右方的随机变量部分参与梯度计算。</li> <li><strong>第三列</strong>：对第一列表达式直接<strong>求梯度</strong>（在代码意义下）得到的结果。</li> <li><strong>第二、四列</strong>：为便于理论分析，分别对第一、三列中的表达式<strong>求期望</strong>（此时采样分布生效），以进一步化简。若它们的期望与真实的KL散度目标及梯度的期望一致，则是无偏的。</li> <li><strong>表头蓝色箭头</strong>说明了列间关系（求梯度或求期望）。</li> <li>简言之，<strong>第一列求梯度时须忽略 $\mathbb{E}$ 下的分布 $q(x)$，而第二、四列求期望时则须让 $q(x)$ 生效。</strong></li> </ul> </li> <li>本文工作建立在诸多前人研究基础上，相关文献列于文末，大部分公式推导均可从中找到。</li> </ol> <h2 id="kl散度作为损失朴素同策略实现">KL散度作为损失（朴素同策略实现）</h2> <ol> <li>在同策略下，采样分布即为KL散度分子部分的分布 $q$。为强调其在代码反向传播中<strong>不被</strong>求梯度，<strong>只对 $k_*$ 求梯度</strong>，表格第一列前三行用 \(\mathbb{E}_{sg(q)}[·]\) 表示。</li> <li>第一行对k1推导，对 \(\mathbb{E}_{sg(q)}[k_1]\) 求梯度，得到第三列的 \(\mathbb{E}_q[s]\)。第二列是第一列的数学期望，结果为 $\mathbb{E}_q [k_1]$，与真实KL散度目标项一致，故目标<strong>无偏</strong>。第四列是第三列的数学期望，结果为 $0$（这是一个重要性质）。公式推导需将期望还原为积分形式，此处仅分析结果：$0$ 与真实梯度项 $\mathbb{E}_q [s k_1]$ 不一致，故梯度<strong>有偏</strong>。</li> <li>第二行推导类似。因 $k_2 = \frac{1}{2} k_1^2$，其第三列梯度结果恰好是 $\mathbb{E}_{sg(q)}[s k_1]$，其期望（第四列）为 $\mathbb{E}_q [s k_1]$，与真实梯度项一致，故梯度<strong>无偏</strong>。</li> <li>第三行推导前面类似，都是<strong>只对 $k_*$ 求梯度</strong>，但需注意其第四列梯度结果是正向KL散度的梯度，与真实（逆向KL）梯度项不一致，故<strong>有偏</strong>。</li> <li><strong>小结</strong>：$k_1$/$k_3$ 作为目标时无偏，但其梯度有偏；$k_2$ 则相反，目标有偏（但偏差低），梯度无偏。</li> </ol> <h2 id="kl散度作为损失统一同异策略实现">KL散度作为损失（统一同/异策略实现）</h2> <ol> <li>在异策略下，采样分布 $\mu$ 不必与目标分布 $q$ 相同，只需在随机变量部分乘上重要性权重 $\frac{q}{\mu}$ 即可。$\mu$ 不被求梯度，而 $q$ 可以。因此可以<strong>同时对 $q$ 与 $k_*$ 求梯度</strong>，可同时得到路径梯度和得分函数，分析更为顺利。</li> <li>若令 $\mu = sg(q)$，则转换为一种可对 $q$ 求梯度的特殊同策略形式。因此，此形式可统一分析同策略与异策略。</li> <li>第四行推导：对第一列k1的表达式求梯度得到第三列（<strong>同时对 $q$ 与 $k_*$ 求梯度</strong>），再求期望得到第四列（利用 $\mathbb{E}_q [s] = 0$ 的性质），结果无偏。</li> <li>第五行推导类似，发现梯度求期望后无法化简，有偏。</li> <li>第六行推导类似，梯度求期望后无偏。</li> <li><strong>小结</strong>：$k_1$/$k_3$ 无论作为目标还是求梯度，都<strong>无偏</strong>；$k_2$ 无论作为目标还是求梯度，都<strong>有偏</strong>。</li> </ol> <h2 id="kl散度作为奖励惩罚统一同异策略实现">KL散度作为奖励惩罚（统一同/异策略实现）</h2> <ol> <li>奖励在目标函数中同样需乘以重要性权重 $\frac{q}{\mu}$。关键区别在于，奖励项本身不作为函数被求梯度，因此作<strong>为奖励惩罚的KL散度整体不被求梯度</strong>。故表格第一列后三行用 $\mathbb{E}_{\mu} \left[ \frac{q}{\mu} sg(\cdot) \right]$ 表示，仅重要性权重中的分子 $q$ 能被求梯度。</li> <li>若令 $\mu = sg(q)$，则转换为可对 $q$ 求梯度的同策略形式，同时保持了理论分析的统一性。</li> <li>第七行推导：对第一列k1的表达式求梯度得到第三列（<strong>只对 $q$ 求梯度</strong>），再求期望得到第四列，结果无偏。</li> <li>第八第九行推导类似，均只对 $q$ 求梯度，$k_*$ 的形式不变，得到梯度是有偏的。</li> <li><strong>小结</strong>：$k_1$ 作为目标与求梯度均<strong>无偏</strong>；$k_2$/$k_3$ 求梯度均<strong>有偏</strong>。</li> </ol> <h2 id="对目标统一进行分析">对目标统一进行分析</h2> <ol> <li>$k_1$, $k_2$, $k_3$ 无论是作为损失还是奖励，无论同策略或异策略，其<strong>目标值</strong>的行为是一致的。</li> <li><strong>偏差角度</strong>：$k_1$/$k_3$ 无偏，$k_2$ 低偏差。</li> <li><strong>方差角度</strong>：$k_1$ 高方差，$k_2$/$k_3$ 低方差。因为 $k_1$ 在0附近是一阶近似，而 $k_2$ 和 $k_3$ 是二阶近似。</li> </ol> <h2 id="对梯度统一进行分析">对梯度统一进行分析</h2> <ol> <li>以下三种情况梯度行为一致，均<strong>无偏</strong>且方差相同，可作为推荐策略： <ul> <li>$k_1$ 作为奖励时</li> <li>$k_2$ 作为损失时（朴素同策略实现）</li> <li>$k_3$ 作为损失时（统一同/异策略实现）</li> </ul> </li> <li>$k_1$ 作为损失时（统一同/异策略实现），其梯度虽与上述情况一致（无偏），但<strong>方差更高</strong>（多出一项零偏差非零方差的项）。</li> <li>除以上列出的情况外，其他使用方式在梯度上均存在偏差，属于<strong>错误方法</strong>。</li> </ol> <h2 id="参考文献">参考文献</h2> <ol> <li> <p>Xihuai Wang, Shao Zhang. “Understanding KL Divergence Estimators in RL: From Value Approximation to Gradient Estimation”. <a href="https://xihuai18.github.io/reinforcement-learning/2025/12/01/kl-estimators-en.html">https://xihuai18.github.io/reinforcement-learning/2025/12/01/kl-estimators-en.html</a></p> </li> <li> <p>Dibya Ghosh. “KL Divergence for Machine Learning”. <a href="https://dibyaghosh.com/blog/probability/kldivergence">https://dibyaghosh.com/blog/probability/kldivergence</a></p> </li> <li> <p>John Schulman. “Approximating KL Divergence”. <a href="https://joschu.net/blog/kl-approx.html">https://joschu.net/blog/kl-approx.html</a></p> </li> <li> <p>Kezhao Liu, Jason Klein Liu, Mingtao Chen, Yiming Liu. “Rethinking KL Regularization in RLHF: From Value Estimation to Gradient Optimization”. <a href="https://arxiv.org/abs/2510.01555">https://arxiv.org/abs/2510.01555</a></p> </li> <li> <p>Yifan Zhang, Yiping Ji, Gavin Brown, et al. “On the Design of KL-Regularized Policy Gradient Algorithms for LLM Reasoning”. <a href="https://arxiv.org/abs/2505.17508">https://arxiv.org/abs/2505.17508</a></p> </li> <li> <p>Vedant Shah, Johan Obando-Ceron, Vineet Jain, Brian Bartoldson, Bhavya Kailkhura, Sarthak Mittal, Glen Berseth, Pablo Samuel Castro. “A Comedy of Estimators: On KL Regularization in RL Training of LLMs”. <a href="https://arxiv.org/abs/2512.21852">https://arxiv.org/abs/2512.21852</a></p> </li> </ol>]]></content><author><name></name></author><category term="reinforcement-learning"/><summary type="html"><![CDATA[一张表串讲LLM-RL中KL散度正则的正确与错误用法]]></summary></entry><entry><title type="html">一图串讲GRPO十几种主流变体算法</title><link href="https://longxinchen-ai.github.io/blog/2026/GRPO-Variants-Formula-Overview-zh/" rel="alternate" type="text/html" title="一图串讲GRPO十几种主流变体算法"/><published>2026-02-10T21:01:00+00:00</published><updated>2026-02-10T21:01:00+00:00</updated><id>https://longxinchen-ai.github.io/blog/2026/GRPO-Variants-Formula-Overview-zh</id><content type="html" xml:base="https://longxinchen-ai.github.io/blog/2026/GRPO-Variants-Formula-Overview-zh/"><![CDATA[<div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/GRPO-Variants.JPG" sizes="95vw"/> <img src="/assets/img/GRPO-Variants.JPG" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong>事先声明</strong>：本文主要从目标公式的角度，尝试归纳并梳理GRPO算法的一些主流变体。为突出重点，部分背景知识已略去，同时涵盖的内容也难免有疏漏之处，还望读者理解。</p> <hr/> <p><strong>图片说明</strong>：</p> <ol> <li><strong>黑色公式</strong>：GRPO 的完整目标函数，尽量沿用原论文符号。</li> <li><strong>蓝色与橙色</strong>：分别表示改进思路与对应的算法名称。</li> <li><strong>采样细节</strong>：公式中期望 $\mathbb{E}$ 内的采样来自行为策略 $\mu_{\text{old}}$ ，而非参考策略 $\pi_{\text{old}}$，这是为了强调训练与推理时的不一致性。后续不少算法都针对此问题进行了改进。</li> <li><strong>DAPO的采样改进</strong>：DAPO 在采样阶段过滤了全 0、全 1 的样本，因为这些样本对梯度更新没有贡献。它也排除了因 <code class="language-plaintext highlighter-rouge">max_len</code> 截断的样本，认为这些样本语义不完整，用于更新只会引入噪声。</li> <li><strong>DAPO的求和改进</strong>：原 GRPO 公式先对序列内的 token 权重取平均，这在一定程度上降低了长序列样本中每个 token 的总体重要性。DAPO 对此进行了改进，对不同长度序列中的 token 权重一视同仁。</li> <li><strong>求和与连乘</strong>：仔细观察 token 级别的求和符号 $\sum$，其实它是有偏差的。熟悉传统序列模型（如 CRF、HMM）的读者知道，它们对整体序列概率的建模一般采用连乘 $\prod$ 形式，而这里的连加 $\sum$ 显得有些“不自然”。可以证明，连加形式仅是连乘形式的一阶泰勒近似。因此，<strong>GSPO</strong> 与 <strong>GMPO</strong> 都选择了连乘形式建模，从而降低了理论偏差。两者的细微区别在于：GMPO 先裁剪后连乘，而 GSPO 先连乘后裁剪。</li> <li><strong>训练-推理不一致的改进</strong>：公式中的 $\frac{\pi_{\text{old}}}{\mu_{\text{old}}}$ 是训练与推理不一致的关键环节，模型训练崩溃常与此有关。该比值越偏离 1，训练越不稳定。各改进策略如下： <ul> <li><strong>IcePOP</strong>：类似 Clip 操作，直接舍弃显著偏离的 token。</li> <li><strong>TIS</strong>：对该比值进行平滑。</li> <li><strong>MIS/WTRS</strong>：对整个序列的训推不一致指标进行评估，并舍弃严重不一致的整条序列。</li> </ul> </li> <li><strong>Min-Clip 操作</strong>：“min-clip” 是作者对 min 与 clip 操作的简写。进一步分析，对 $\frac{\pi_\theta}{\pi_{\text{old}}}$ 直接进行硬裁剪会浪费本就不多的有效样本，不利于探索。因此，<strong>CISPO</strong> 和 <strong>SAPO</strong> 都致力于对 clip 操作进行平滑化改进。这种改进思路类似于 Hinge Loss 与 Log Loss 的区别。</li> <li><strong>CISPO 思路</strong>：保留梯度，引入停止梯度（stop-gradient, sg）操作。为保证在非裁剪区域的梯度一致，增加了 $\log(\pi_\theta)$ 项进行配平。</li> <li><strong>SAPO 思路</strong>：直接使用 Log Loss 中的 Sigmoid 函数进行平滑，其中 $\tau$ 为温度系数。为确保 $\frac{\pi_\theta}{\pi_{\text{old}}}$ 在 1 附近的梯度不变，引入了 $\frac{4}{\tau}$ 进行配平。</li> <li><strong>小结</strong>： <ul> <li>$\frac{\pi_\theta}{\pi_{\text{old}}}$ 关注的是 <strong>策略新旧不一致</strong>。裁剪太少会使代理目标偏离真实目标，导致训练崩溃；裁剪太多则会抑制探索，容易过拟合。</li> <li>$\frac{\pi_{\text{old}}}{\mu_{\text{old}}}$ 关注的是 <strong>训练与推理不一致</strong>。裁剪太少易导致训练崩溃，裁剪太多也可能不利于探索。</li> <li>两者作用不同，可以解耦处理。现有改进策略多在 <strong>裁剪方式</strong> 上做文章，例如：硬裁剪 vs 软裁剪、Token 级裁剪 vs 序列级裁剪、对称裁剪 vs 非对称裁剪。例如，DAPO 建议直接提高裁剪上限，就是一种典型的非对称裁剪。</li> </ul> </li> <li><strong>补充说明</strong>：CISPO 和 SAPO 都是对 min-clip 中的 clip 部分进行改进，而 <strong>舍弃了 min 操作</strong>。这并非不合理，但会导致它们的函数图像在部分区域与 min-clip 的图像趋势不一致，分析时请注意。</li> <li><strong>针对 MOE 模型的改进</strong>：对于混合专家模型，上述两种不一致性会被路由机制显著放大。注意到 $\frac{\pi_\theta}{\mu_{\text{old}}}=\frac{\pi_\theta}{\pi_{\text{old}}} \times \frac{\pi_{\text{old}}}{\mu_{\text{old}}}$。由此产生两种思路： <ul> <li><strong>R2</strong>：将 $\pi_\theta$ 中的专家固定为 $\pi_{\text{old}}$ 中的专家，以降低新旧不一致。</li> <li><strong>R3</strong>：将 $\pi_\theta$ 中的专家约束为可学习的，但使其趋向于 $\mu_{\text{old}}$ 中的专家，以同时降低两种不一致性。</li> <li>需要注意的是，这些路由回放策略也降低了模型的容量，可能损害其拟合性能。可以理解成拿偏差换方差。</li> </ul> </li> <li><strong>奖励分数估计</strong>：原始 GRPO 公式中的归一化分母（标准差 std）可证明是有偏的，因此 <strong>Dr.GRPO</strong> 选择直接去掉它。</li> <li><strong>奖励塑形</strong>：针对单个样本的奖励设计也有不少工作，例如 <strong>DAPO</strong> 专门为过长的序列样本设计了一个软性惩罚，实验证明有一定效果。</li> <li><strong>关于 KL 散度正则项</strong>：本文分析未包含 KL 散度正则项，主要将其视为与原始目标函数解耦的一个独立策略进行处理。事实上，部分论文会直接放弃此项。由于 KL 散度有多种估计和应用方式，笔者将另文介绍。</li> <li>高清原图可到github上代码仓库下载。</li> </ol>]]></content><author><name></name></author><category term="reinforcement-learning"/><summary type="html"><![CDATA[一图串讲GRPO十几种主流变体算法]]></summary></entry></feed>