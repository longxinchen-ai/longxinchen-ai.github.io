<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> 一张表串讲LLM-RL中KL散度正则的正确与错误用法 | Longxinchen's Page </title> <meta name="author" content="Long Xinchen"> <meta name="description" content="一张表串讲LLM-RL中KL散度正则的正确与错误用法"> <meta name="keywords" content="Reinforcement Learning, Multi-agent System, Language Model, AI, Machine Learning"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/prof_pic_icon.jpg?v=5c18269c5d70c42163dd3f354120a548"> <link rel="stylesheet" href="/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://longxinchen-ai.github.io/blog/2026/KL-Reg-zh/"> <script src="/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Longxinchen's Page </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">一张表串讲LLM-RL中KL散度正则的正确与错误用法</h1> <p class="post-meta"> Created on February 11, 2026 </p> <p class="post-tags"> <a href="/blog/2026"> <i class="fa-solid fa-calendar fa-sm"></i> 2026 </a>   ·   <a href="/blog/category/reinforcement-learning"> <i class="fa-solid fa-tag fa-sm"></i> reinforcement-learning</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/KL-Reg.PNG" sizes="95vw"></source> <img src="/assets/img/KL-Reg.PNG" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p><strong>声明</strong>：为突出重点，部分背景知识已略去，同时涵盖的内容也难免有疏漏之处，还望读者理解。</p> <hr> <h2 id="背景说明">背景说明</h2> <ol> <li> <p>KL散度正则化涉及多种决策项，包括：使用正向KL散度还是逆向KL散度（2种）、将其置于损失函数还是奖励函数中（2种）、采用同策略还是异策略（2种）、选择k1/k2/k3中的哪种形式（3种）、以及分析目标和梯度各自的偏差与方差（2×2种）。简单计算有 $2×2×2×3×2×2=96$ 种组合。当然，实际分析无需覆盖全部。例如，本文聚焦于当前主流的逆向KL散度，情况数便减少一半。若偏差分析已证实存在偏差，方差分析也可省略。</p> </li> <li> <p>无论作何选择，<strong>最关键的是保证实际训练中KL散度梯度的偏差和方差足够小。而对KL散度求梯度，是整个分析中最需留意的关键点。</strong></p> </li> <li> <p>从数学上讲，KL散度本质是一个期望，而期望的本质是积分。对KL散度求梯度，就是对积分求梯度，这需要用到莱布尼茨积分求导法则。具体来说，已知随机变量 $f(x; \theta)$ 在分布 $q(x; \theta)$ 下的期望 $\mathbb{E}_{x \sim q(x; \theta)}[f(x; \theta)]$，对其求参数 $\theta$ 的梯度，需<strong>分别对 $f$ 和 $q$ 求梯度</strong>，结果分别称为“路径梯度”和“得分函数”。公式表达为： \(\nabla_{\theta} \mathbb{E}_{x \sim q(x; \theta)}[f(x; \theta)] = \mathbb{E}_{x \sim q(x; \theta)} \left[ \nabla_{\theta} f(x; \theta) + f(x; \theta) \, \nabla_{\theta} \log q(x; \theta) \right]\)</p> </li> <li> <p>对于逆向KL散度 $\mathbb{E}_q [k_1]$，套用上述梯度公式，其路径梯度项为零，仅剩得分函数项 $\mathbb{E}_q [s k_1]$。这两项即表格中第0行列出的真实（True）KL散度目标项（第二列）与梯度项（第四列）。</p> </li> <li> <p>在代码实现中，对期望（无论损失还是梯度）的计算均采用蒙特卡洛估计。期望符号 $\mathbb{E}$ 右下角的分布 $q(x)$ 仅用于采样，并未在计算图中显式出现。这对损失值计算影响不大，但求梯度时则不同：反向传播不会对采样分布 $q(x)$ 求梯度，因而会遗漏重要的得分函数项。因此，核心问题在于，<strong>确保代码实现与理论分析两种方式求得的梯度尽量相等（无偏）</strong>，并在此基础上降低方差。</p> </li> <li>为阐明此问题，表格分为4列： <ul> <li> <strong>第一列</strong>：代码实现意义上的目标表达式。此列中所有期望 $\mathbb{E}$ 右下角的采样分布<strong>不会</strong>被求梯度，仅正右方的随机变量部分参与梯度计算。</li> <li> <strong>第三列</strong>：对第一列表达式直接<strong>求梯度</strong>（在代码意义下）得到的结果。</li> <li> <strong>第二、四列</strong>：为便于理论分析，分别对第一、三列中的表达式<strong>求期望</strong>（此时采样分布生效），以进一步化简。若它们的期望与真实的KL散度目标及梯度的期望一致，则是无偏的。</li> <li> <strong>表头蓝色箭头</strong>说明了列间关系（求梯度或求期望）。</li> <li>简言之，<strong>第一列求梯度时须忽略 $\mathbb{E}$ 下的分布 $q(x)$，而第二、四列求期望时则须让 $q(x)$ 生效。</strong> </li> </ul> </li> <li>本文工作建立在诸多前人研究基础上，相关文献列于文末，大部分公式推导均可从中找到。</li> </ol> <h2 id="kl散度作为损失朴素同策略实现">KL散度作为损失（朴素同策略实现）</h2> <ol> <li>在同策略下，采样分布即为KL散度分子部分的分布 $q$。为强调其在代码反向传播中<strong>不被</strong>求梯度，<strong>只对 $k_*$ 求梯度</strong>，表格第一列前三行用 \(\mathbb{E}_{sg(q)}[·]\) 表示。</li> <li>第一行对k1推导，对 \(\mathbb{E}_{sg(q)}[k_1]\) 求梯度，得到第三列的 \(\mathbb{E}_q[s]\)。第二列是第一列的数学期望，结果为 $\mathbb{E}_q [k_1]$，与真实KL散度目标项一致，故目标<strong>无偏</strong>。第四列是第三列的数学期望，结果为 $0$（这是一个重要性质）。公式推导需将期望还原为积分形式，此处仅分析结果：$0$ 与真实梯度项 $\mathbb{E}_q [s k_1]$ 不一致，故梯度<strong>有偏</strong>。</li> <li>第二行推导类似。因 $k_2 = \frac{1}{2} k_1^2$，其第三列梯度结果恰好是 $\mathbb{E}_{sg(q)}[s k_1]$，其期望（第四列）为 $\mathbb{E}_q [s k_1]$，与真实梯度项一致，故梯度<strong>无偏</strong>。</li> <li>第三行推导前面类似，都是<strong>只对 $k_*$ 求梯度</strong>，但需注意其第四列梯度结果是正向KL散度的梯度，与真实（逆向KL）梯度项不一致，故<strong>有偏</strong>。</li> <li> <strong>小结</strong>：$k_1$/$k_3$ 作为目标时无偏，但其梯度有偏；$k_2$ 则相反，目标有偏（但偏差低），梯度无偏。</li> </ol> <h2 id="kl散度作为损失统一同异策略实现">KL散度作为损失（统一同/异策略实现）</h2> <ol> <li>在异策略下，采样分布 $\mu$ 不必与目标分布 $q$ 相同，只需在随机变量部分乘上重要性权重 $\frac{q}{\mu}$ 即可。$\mu$ 不被求梯度，而 $q$ 可以。因此可以<strong>同时对 $q$ 与 $k_*$ 求梯度</strong>，可同时得到路径梯度和得分函数，分析更为顺利。</li> <li>若令 $\mu = sg(q)$，则转换为一种可对 $q$ 求梯度的特殊同策略形式。因此，此形式可统一分析同策略与异策略。</li> <li>第四行推导：对第一列k1的表达式求梯度得到第三列（<strong>同时对 $q$ 与 $k_*$ 求梯度</strong>），再求期望得到第四列（利用 $\mathbb{E}_q [s] = 0$ 的性质），结果无偏。</li> <li>第五行推导类似，发现梯度求期望后无法化简，有偏。</li> <li>第六行推导类似，梯度求期望后无偏。</li> <li> <strong>小结</strong>：$k_1$/$k_3$ 无论作为目标还是求梯度，都<strong>无偏</strong>；$k_2$ 无论作为目标还是求梯度，都<strong>有偏</strong>。</li> </ol> <h2 id="kl散度作为奖励惩罚统一同异策略实现">KL散度作为奖励惩罚（统一同/异策略实现）</h2> <ol> <li>奖励在目标函数中同样需乘以重要性权重 $\frac{q}{\mu}$。关键区别在于，奖励项本身不作为函数被求梯度，因此作<strong>为奖励惩罚的KL散度整体不被求梯度</strong>。故表格第一列后三行用 $\mathbb{E}_{\mu} \left[ \frac{q}{\mu} sg(\cdot) \right]$ 表示，仅重要性权重中的分子 $q$ 能被求梯度。</li> <li>若令 $\mu = sg(q)$，则转换为可对 $q$ 求梯度的同策略形式，同时保持了理论分析的统一性。</li> <li>第七行推导：对第一列k1的表达式求梯度得到第三列（<strong>只对 $q$ 求梯度</strong>），再求期望得到第四列，结果无偏。</li> <li>第八第九行推导类似，均只对 $q$ 求梯度，$k_*$ 的形式不变，得到梯度是有偏的。</li> <li> <strong>小结</strong>：$k_1$ 作为目标与求梯度均<strong>无偏</strong>；$k_2$/$k_3$ 求梯度均<strong>有偏</strong>。</li> </ol> <h2 id="对目标统一进行分析">对目标统一进行分析</h2> <ol> <li>$k_1$, $k_2$, $k_3$ 无论是作为损失还是奖励，无论同策略或异策略，其<strong>目标值</strong>的行为是一致的。</li> <li> <strong>偏差角度</strong>：$k_1$/$k_3$ 无偏，$k_2$ 低偏差。</li> <li> <strong>方差角度</strong>：$k_1$ 高方差，$k_2$/$k_3$ 低方差。因为 $k_1$ 在0附近是一阶近似，而 $k_2$ 和 $k_3$ 是二阶近似。</li> </ol> <h2 id="对梯度统一进行分析">对梯度统一进行分析</h2> <ol> <li>以下三种情况梯度行为一致，均<strong>无偏</strong>且方差相同，可作为推荐策略： <ul> <li>$k_1$ 作为奖励时</li> <li>$k_2$ 作为损失时（朴素同策略实现）</li> <li>$k_3$ 作为损失时（统一同/异策略实现）</li> </ul> </li> <li>$k_1$ 作为损失时（统一同/异策略实现），其梯度虽与上述情况一致（无偏），但<strong>方差更高</strong>（多出一项零偏差非零方差的项）。</li> <li>除以上列出的情况外，其他使用方式在梯度上均存在偏差，属于<strong>错误方法</strong>。</li> </ol> <h2 id="参考文献">参考文献</h2> <ol> <li> <p>Xihuai Wang, Shao Zhang. “Understanding KL Divergence Estimators in RL: From Value Approximation to Gradient Estimation”. <a href="https://xihuai18.github.io/reinforcement-learning/2025/12/01/kl-estimators-en.html" rel="external nofollow noopener" target="_blank">https://xihuai18.github.io/reinforcement-learning/2025/12/01/kl-estimators-en.html</a></p> </li> <li> <p>Dibya Ghosh. “KL Divergence for Machine Learning”. <a href="https://dibyaghosh.com/blog/probability/kldivergence" rel="external nofollow noopener" target="_blank">https://dibyaghosh.com/blog/probability/kldivergence</a></p> </li> <li> <p>John Schulman. “Approximating KL Divergence”. <a href="https://joschu.net/blog/kl-approx.html" rel="external nofollow noopener" target="_blank">https://joschu.net/blog/kl-approx.html</a></p> </li> <li> <p>Kezhao Liu, Jason Klein Liu, Mingtao Chen, Yiming Liu. “Rethinking KL Regularization in RLHF: From Value Estimation to Gradient Optimization”. <a href="https://arxiv.org/abs/2510.01555" rel="external nofollow noopener" target="_blank">https://arxiv.org/abs/2510.01555</a></p> </li> <li> <p>Yifan Zhang, Yiping Ji, Gavin Brown, et al. “On the Design of KL-Regularized Policy Gradient Algorithms for LLM Reasoning”. <a href="https://arxiv.org/abs/2505.17508" rel="external nofollow noopener" target="_blank">https://arxiv.org/abs/2505.17508</a></p> </li> <li> <p>Vedant Shah, Johan Obando-Ceron, Vineet Jain, Brian Bartoldson, Bhavya Kailkhura, Sarthak Mittal, Glen Berseth, Pablo Samuel Castro. “A Comedy of Estimators: On KL Regularization in RL Training of LLMs”. <a href="https://arxiv.org/abs/2512.21852" rel="external nofollow noopener" target="_blank">https://arxiv.org/abs/2512.21852</a></p> </li> </ol> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2026/GRPO-Variants-Formula-Overview-zh/">一图串讲GRPO十几种主流变体算法</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Long Xinchen. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script defer src="/assets/js/google-analytics-setup.js"></script> <script defer src="/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>